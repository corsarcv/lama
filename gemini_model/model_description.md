# Gemini Model Description
## Version 1
**Constants:** Define filenames, minimum data requirements (MIN_TRAIN_RECORDS_PER_STOCK, MIN_HISTORY_FOR_PREDICTION), LSTM sequence length (N_STEPS), and suggestion thresholds (THRESHOLDS).
N_FEATURES tells the script and the LSTM model how many distinct input variables to expect for every single point in the time sequence being analyzed. In this initial version, it's 1 because we are solely focusing on the historical 'price'.

**StockSuggester Class**:
- __init__: Initializes paths, loads data using _load, sets is_sufficiently_trained flag.
- _load: Handles loading the Keras model (load_model), Pandas history (CSV), and Scikit-learn scalers (joblib.load). Includes error handling.
- _save_history, _save_model, _save_scalers: Handles saving the respective data to files.
- _prepare_data_for_stock: Takes a DataFrame for a single stock, scales its price data using MinMaxScaler, creates input sequences (X) and target values (y) suitable for the LSTM, and returns X, y, and the scaler used. This per-stock scaling is crucial.
- learn(events):
    - Takes new events, validates them, and adds them to the history DataFrame.
    - Saves the updated history.
    - Iterates through each unique stock in the history.
    - If a stock has enough data (MIN_TRAIN_RECORDS_PER_STOCK), it calls _prepare_data_for_stock.
    - Collects X, y, and scalers from all eligible stocks.
    - If any stock provided sufficient data, it concatenates X and y from all stocks.
    - Reshapes X for the LSTM input shape (samples, timesteps, features).
    - Builds a new LSTM model if one doesn't exist, or uses the existing one. The model is simple: two LSTM layers and two Dense layers.
    - Compiles the model (Adam optimizer, MSE loss).
    - Trains (model.fit) the model on the combined, scaled data.
    - Sets is_sufficiently_trained to True.
    - Saves the updated model and the collection of scalers (one for each stock trained).
- predict(events):
    - Checks if the model is trained (is_sufficiently_trained). If not, returns "insufficient learning" for all input stocks.
    - Takes recent events, combines them with history.
    - Iterates through the unique stocks in the input events.
    - Checks if a scaler exists for the stock (it must have been seen and processed during learn).
    - Checks if the stock has enough historical data points (MIN_HISTORY_FOR_PREDICTION) in the combined history. If not, returns "insufficient history".
    - Selects the last N_STEPS prices for the stock.
    - Uses the specific scaler for that stock (loaded during __init__ or updated during learn) to transform this sequence.
    - Reshapes the sequence for prediction.
    - Calls model.predict.
    - Inverse transforms the scaled prediction back to a price value using the same scaler.
    - Calculates the percentage change between the predicted price and the last actual price in the sequence.
    - Compares the change to THRESHOLDS to determine the suggestion ('strong_buy', 'buy', 'hold', 'sell', 'strong_sell').
    - Returns a dictionary of suggestions.
- Example Usage (if __name__ == "__main__":)
    - Creates an instance of StockSuggester.
    - Generates synthetic data for learning (Stock A, B, C with varying trends and amounts of data).
    - Calls learn with the initial data.
    - Calls learn again with more data for Stock C to show incremental learning.
    - Generates synthetic recent events for prediction.
    - Calls predict and prints the results.
    - Includes examples for testing "insufficient history" and "insufficient learning" scenarios.

## Version 2. Key Changes and Explanations:

- Import pandas_ta: Added the library for calculating indicators.
- Constants Updated:
    - N_FEATURES is now 6.
    - FEATURE_COLS: Defined the exact list and order of feature columns used by the model.
    - PRICE_INDEX: Automatically finds the index of 'price' in FEATURE_COLS, crucial for scaling/inverse scaling and extracting the target variable y.
    - Indicator parameters (EMA_PERIOD, MACD_..., RSI_PERIOD) added.
    - Minimum record counts (MIN_RECORDS_FOR_INDICATORS, MIN_TRAIN_RECORDS_PER_STOCK, MIN_HISTORY_FOR_PREDICTION) adjusted to account for NaNs produced by initial indicator calculations.
- History DataFrame: __init__ and _load now handle the volume and moving_average columns.
- _calculate_indicators(stock_data):
    - New private method dedicated to calculating EMA, MACD, and RSI using pandas_ta.
    - Takes a DataFrame for a single stock.
    - Uses df.ta.indicator_name(...) to append indicator columns directly.
    - Handles required column names automatically generated by pandas_ta.
    - Crucially, it dropna() to remove rows where indicators couldn't be calculated (usually the first few rows).
    - Selects and returns only the columns specified in FEATURE_COLS, ensuring the correct order.
    - Includes checks for sufficient data before calculation.
- _prepare_data_for_stock(stock_data):
    - Calls _calculate_indicators first.
    - Checks for sufficient data after indicators are calculated and NaNs are dropped.
    - Selects the FEATURE_COLS for scaling.
    - MinMaxScaler now scales all 6 features together for each stock.
    - X sequences now contain N_STEPS time points, each with N_FEATURES values.
    - y target remains the single scaled price value for the next step, extracted using PRICE_INDEX.
- learn(events):
    - Expects input events with 5 items: (time, stock, price, volume, moving_average).
    - Validates and processes these new inputs.
    - Updates history with the new columns.
    - Calls the updated _prepare_data_for_stock.
    - Model Input layer Input(shape=(N_STEPS, N_FEATURES)) correctly reflects the 6 features.
    - Model architecture slightly adjusted (more units in LSTM/Dense) to potentially handle more complex patterns from 6 features (optional, can be tuned).
    - Training data shapes (X_train, y_train) are printed for verification.
- predict(events):
    - Expects recent events with the 5 items.
    - Combines recent data and history.
    - Calls _calculate_indicators on the combined data for the specific stock being predicted.
    - Checks for sufficient history length after indicator calculation.
    - Selects the last N_STEPS rows of all FEATURE_COLS.
    - Gets the last_actual_price from the unscaled data.
    - Scales the 6-feature sequence using the loaded scaler for that stock.
    - Inverse Scaling: This is important. The model predicts a single scaled value (the price). To inverse transform it correctly using the multi-feature scaler, we create a dummy array of shape (1, N_FEATURES), place the predicted scaled price at the PRICE_INDEX, and then call scaler.inverse_transform. We then extract the value at PRICE_INDEX from the result to get the predicted price.
    - Calculates percentage change and determines the suggestion.
- Example Usage:
    - Simulated data generation now includes dummy volume and moving_average values.
    - More historical data points (300) are generated to allow indicators to stabilize.
    - Prediction example tries to realistically continue the trends and calculates a rolling MA for the simulated recent events.
    - Added prediction for STOCK_D (insufficient history) and STOCK_NEW (unknown to model/no scaler).
    - This updated script now leverages multiple features, potentially leading to more nuanced and accurate predictions, although model tuning and feature engineering become even more critical. Remember that the quality of the input moving_average also affects performance.

## Model Learning Principles
In the provided script, the model employs a hybrid approach:

1. **Shared Core Model**: There is only one underlying LSTM model (self.model). This single model is trained on data sequences concatenated from all stocks that meet the MIN_TRAIN_RECORDS_PER_STOCK threshold during the learn phase.
- X_train = np.concatenate(all_X, axis=0)
- y_train = np.concatenate(all_y, axis=0)
- self.model.fit(X_train, y_train, ...)
- Implication: The model attempts to learn generalizable patterns and relationships between the input features (price change, volume change, indicator movements, etc.) that might be common across different stocks in the dataset. Patterns learned from STOCK_A's behavior can theoretically influence how the model interprets STOCK_B's input during prediction.
2. **Stock-Specific Scaling**: Crucially, the data scaling (using MinMaxScaler) is performed independently for each stock.
- A separate scaler object is created and fitted for each stock during _prepare_data_for_stock.
- These stock-specific scalers are stored in the self.scalers dictionary (self.scalers[stock] = scaler).
- Implication: This step is vital. It normalizes the features (price, volume, MA, indicators) for each stock based on its own historical range. A $1 price change for a $10 stock is treated differently than a $1 change for a $1000 stock before scaling, but after scaling, similar percentage movements or movements relative to their typical range might look similar to the model. When predicting, the input sequence for a specific stock is scaled using its own scaler, and the model's output (a scaled value) is inverse-transformed back into a price using that same stock-specific scaler.

### In summary:

- Learning: The LSTM weights are adjusted based on patterns observed across all included stocks' (scaled) data.
- Prediction: The single, globally trained model is used, but it operates on input data that has been scaled specifically for the target stock, and its output is interpreted using that same stock-specific scaling information.

### Advantages of this approach:

- **More Data**: The model potentially learns more robust patterns by seeing data from multiple stocks, especially beneficial if some individual stocks have limited history (but still meet the minimum).
- **Generalization**: Aims to capture market-wide or sector-wide patterns if they exist and are reflected across the stocks provided.


## Disadvantages/Considerations:

- **Assumption of Transferability:** It assumes that patterns learned from one stock are somewhat relevant to others. If stocks behave very differently (e.g., different sectors, market caps, volatility profiles), a single model might struggle to capture all nuances perfectly.
- **Averaging Effect:** The model might average out highly specific behaviors unique to a single stock.


### Alternative:

The alternative would be to create, train, and save a completely separate StockSuggester instance or a separate model and scaler within the class for each stock ticker. This would ensure zero data sharing between stocks during model training but would require significantly more resources (memory, disk space, training time) and might perform poorly for stocks with less individual data.


## What is Feature Scaling?


Feature scaling is a crucial preprocessing step in many machine learning algorithms, especially those sensitive to the magnitude of input values, like:

1. Neural Networks (including LSTMs): The optimization algorithms used (like Gradient Descent) converge much faster and more reliably when features are on a similar, smaller scale. Large input values can lead to very large gradients, causing unstable updates or slow convergence. Activation functions also often work best in specific ranges (e.g., 0 to 1).
2. Distance-Based Algorithms: Algorithms like K-Nearest Neighbors (KNN) or Support Vector Machines (SVM) rely on calculating distances between data points. If one feature has a much larger range than others (e.g., price in dollars vs. RSI between 0-100), that feature will dominate the distance calculation, making the contribution of smaller-range features negligible.

### What is a Scaler?

A "scaler" is an object (in our case, from the scikit-learn library) that performs this feature scaling. It learns the necessary parameters from your training data and then applies the transformation to bring your features into the desired range.

**MinMaxScaler (Used in the Script)**

- **What it does:** MinMaxScaler transforms each feature individually such that it falls within a given range, typically [0, 1] (which is the default and what we use in the script).
- **How it works:** For each feature column, it finds the minimum (min) and maximum (max) value in the training data. Then, it transforms each value (x) in that column using the formula:
*x_scaled = (x - min) / (max - min)*
- **Result:** The minimum value in the original feature becomes 0, the maximum value becomes 1, and all other values are linearly scaled to fall between 0 and 1.

### Why Use Scalers in This Stock Prediction Script?

1. **Different Feature Magnitudes:** You have features with vastly different typical ranges:
- price: Could be anywhere from dollars to hundreds or thousands of dollars.
- volume: Can range from hundreds to millions or billions.
- moving_average: Similar range to price.
- EMA: Similar range to price.
- MACD: Values often hover around zero but can fluctuate (e.g., -5 to +5).
- RSI: Strictly bounded between 0 and 100.
Without scaling, the price and volume would completely dominate the input to the LSTM, and the model would struggle to learn the subtle influences of MACD or RSI.
2. **LSTM Sensitivity:** LSTMs, being neural networks, benefit greatly from scaled inputs for stable training and faster convergence.
3. **Stock-Specific Scaling:** This is critical in the script. We create a separate MinMaxScaler for each stock. Why? Because the price range, volume range, etc., of 'STOCK_A' might be completely different from 'STOCK_B'. Scaling them together would lose the context of what a "high" or "low" value means for that specific stock. By fitting a scaler to each stock's historical data, we normalize its features based on its own past behavior.
4. **Inverse Transformation** (inverse_transform): The LSTM model predicts the next scaled price (a value between 0 and 1). This isn't useful on its own. We need the scaler object (the same one used to scale the input for that stock) to reverse the process using inverse_transform. This converts the scaled prediction back into an actual, understandable price value, which we can then compare to the last actual price to calculate the percentage change and generate a suggestion.
5. **Persistence** (joblib.dump/load): Since the exact min and max values learned during training (fitting the scaler) must be used to scale new data during prediction and to inverse-transform the output, the scaler objects themselves need to be saved alongside the model. If you loaded the model but used a fresh scaler (or refit it on new data), the predictions would be meaningless.
**In summary**, scalers (like MinMaxScaler) are essential tools to normalize the range of your input features, making them suitable for algorithms like LSTMs. In this script, they ensure that all 6 features contribute appropriately to the model's learning process and allow for meaningful interpretation of the model's scaled output by converting it back to a real price prediction. The stock-specific application and saving/loading of these scalers are key implementation details.

## Time intervals
Please note than the current **model does not differentiate between changes based on the time interval between events. It only sees the sequence of records**. In order to have accurate predictions, we should feed learning data as well as latest events with the same time period (e. g. 1 hour). Also, we need to load recent history for the stock before asking for a prediction.